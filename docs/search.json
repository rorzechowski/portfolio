{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-06T01:42:47-04:00"
    },
    {
      "path": "blog_list.html",
      "title": "Blogs",
      "description": "Creative blogs from insightful people in the R community.\n",
      "author": [],
      "contents": "\r\nLisa LendwayGreat posts with intro to R courses, building an R Markdown site, and tutorials on GitHub\r\nTed LaderasMostly bioinformatics teaching, but has posts about many topics ranging from mental health to R courses\r\nAlison HillGood advanced R Markdown workshop as well as other fun data sets\r\nAllison HorstAmazing illustrations about many data science topics\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-06T23:09:49-04:00"
    },
    {
      "path": "book_list.html",
      "title": "Library of Learning",
      "description": "Helpful books on a wide variety of topics. \n",
      "author": [],
      "contents": "\r\nData Feminism by Catherine D’Ignazio and Lauren Klein- A powerful book illustrating the historic challenges associated with data science and adapting a new way of framing data visualization techniques in a feminist way.\r\nAn Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani- Textbook read during STA 631. Covers topics such as linear regression, classification, and more. \r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-06T22:57:43-04:00"
    },
    {
      "path": "current_projects.html",
      "title": "Current Projects",
      "description": "A reflection of topics covered in STA 631 - Summer 2023\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nCourse Objective 1\r\nDiscussion & Reflection\r\nCourse Objective 2\r\nDiscussion & Reflection\r\nCourse Objective 3\r\nDiscussion & Reflection\r\nCourse Objective 4\r\nDiscussion & Reflection\r\nCourse Objective 5\r\nDiscussion & Reflection\r\n\r\nCourse Objective 1\r\nDescribe probability as a foundation of statistical modeling, including inference and maximum likelihood estimation\r\nFrom Activity 6 - Logistic Regression\r\n\r\n\r\n#load library\r\nlibrary(tidyverse)\r\nlibrary(tidymodels)\r\n\r\n\r\n\r\n\r\n#read in data from URL\r\nresume <- read_csv(\"https://www.openintro.org/data/csv/resume.csv\")\r\n\r\n\r\n\r\n\r\n#coerce variable of interest to binary\r\nresume$received_callback <- factor(resume$received_callback)\r\n\r\n\r\n\r\n\r\n# probability of getting a callback\r\nprob_yes = 392/4870\r\nprob_yes\r\n\r\n[1] 0.08049281\r\n\r\nodds_yes = (prob_yes/ (1-prob_yes))\r\nodds_yes\r\n\r\n[1] 0.08753908\r\n\r\nLogistic Regression\r\n\r\n\r\nresume_mod <- logistic_reg() %>%\r\n  set_engine(\"glm\") %>%\r\n  fit(received_callback ~ race, data = resume, family = \"binomial\")\r\n\r\ntidy(resume_mod) %>% \r\n  knitr::kable(digits = 3)\r\n\r\nterm\r\nestimate\r\nstd.error\r\nstatistic\r\np.value\r\n(Intercept)\r\n-2.675\r\n0.083\r\n-32.417\r\n0\r\nracewhite\r\n0.438\r\n0.107\r\n4.083\r\n0\r\n\r\n\r\n\r\n#log odds of perceived black person getting callback\r\nlog_odds_callback = -2.68\r\n\r\nodds_callback = exp(log_odds_callback)\r\nodds_callback\r\n\r\n[1] 0.06856315\r\n\r\nDiscussion & Reflection\r\nThe logistic regression assignment was a good way to learn about probability, odds, and the roll they play while building models. In this assignment, we analyzed at a data set that looked at the influence of race and gender on the probability of a person receiving a callback after a job interview solely based on their name. We saw that, despite many of the candidates having very good credentials (i.e. a high number of years of experience), that there were a very low number of callbacks given to anyone. When we further looked at black individuals, we saw that the callback probability decreased even more.\r\nI thought that this data set was enlightening in a few ways. One, that you may disproportionately not be given a job opportunity solely based on your name and its perception of race or gender to another person. That level of discrimination, while illegal, does still occur as revealed by this study. As a female, that knowledge is disheartening. I can’t even image what a person of color might experience. I think it’s important to acknowledge your own biases and actively work to avoid them. This data set, in a roundabout way, helped me to think about that in a more present sense.\r\nCourse Objective 2\r\nDetermine and apply the appropriate generalized linear model for a specific data context\r\nFrom Mini Competition 2\r\n\r\n\r\nlibrary(tidyverse)\r\nraw_data <- read_csv(\"inventory.csv\")\r\n\r\n\r\n\r\n\r\n#convert week data by month #easier for interpretation\r\ndata_month <- raw_data %>% mutate(Month = case_when(week <= 4 ~ 'July',\r\n                                                    week > 4 & week <= 8 ~ 'August',\r\n                                                    week > 8 & week <= 12 ~ 'September',\r\n                                                    week > 12 & week <= 16 ~ 'October',\r\n                                                    week > 16 & week <= 20 ~ 'November',\r\n                                                    week > 20 & week <= 24 ~ 'December',\r\n                                                    week > 24 & week <= 28 ~ 'January',\r\n                                                    week > 28 & week <= 32 ~ 'February',\r\n                                                    week > 32 & week <= 37 ~ 'March',\r\n                                                    week > 37 & week <= 41 ~ 'April',\r\n                                                    week > 41 & week <= 45 ~ 'May',\r\n                                                    week > 45 & week <= 50 ~ 'June',\r\n                                                    week > 50 & week <= 53 ~ 'July2',))\r\n\r\ndata_month$Month <- factor(data_month$Month, levels = c(\"July\", \"August\", \"September\",\r\n                                                        \"October\", \"November\", \"December\",\r\n                                                        \"January\", \"February\", \"March\",\r\n                                                        \"April\", \"May\", \"June\", \"July2\"))\r\ndata_month$item_no <- factor(data_month$item_no)\r\n\r\n\r\n\r\n\r\n#add binary info\r\ndata_month_bi <- data_month %>% mutate(sold_binary = case_when(sold > 0 ~ '1',\r\n                                                    TRUE ~ '0' ))\r\n\r\ndata_month_bi$sold_binary <- as.numeric(data_month_bi$sold_binary)\r\n\r\n\r\n\r\n\r\ndata_wider <- data_month %>% pivot_wider(names_from = item_no,\r\n                                    values_from = sold)\r\n\r\ndata_wider\r\n\r\n# A tibble: 54 × 490\r\n    week Month   `020-307` `020-502` `025-207` `02FR182024` `04002032`\r\n   <dbl> <fct>       <dbl>     <dbl>     <dbl>        <dbl>      <dbl>\r\n 1     0 July            0         0         0            0          0\r\n 2     1 July           80         0       100            0          0\r\n 3     2 July            0         0         0            0          0\r\n 4     3 July            0         0         0            0        284\r\n 5     4 July            0        84         0            0         88\r\n 6     5 August          0         0         0            0        440\r\n 7     6 August          0         0         0            0         60\r\n 8     7 August          0        84         0            0         75\r\n 9     8 August         80         0       100            0          0\r\n10     9 Septem…         0         0         0            0          0\r\n# ℹ 44 more rows\r\n# ℹ 483 more variables: `04120002` <dbl>, `0822203` <dbl>,\r\n#   `10055.011010` <dbl>, `10055.011020` <dbl>, `10055.011212` <dbl>,\r\n#   `10055.011220` <dbl>, `10055.011224` <dbl>, `10055.011420` <dbl>,\r\n#   `10055.011425` <dbl>, `10055.011616` <dbl>, `10055.011620` <dbl>,\r\n#   `10055.011624` <dbl>, `10055.011625` <dbl>, `10055.011820` <dbl>,\r\n#   `10055.012020` <dbl>, `10055.012025` <dbl>, …\r\n\r\n\r\n\r\n#see overall trends across time\r\nggplot(data = data_month, aes(x = Month,\r\n                             y = sold,\r\n                             color = item_no,\r\n                             fill = item_no)) +\r\n  geom_col() + \r\n  theme_bw() +\r\n  theme(panel.grid.major = element_blank(), \r\n        panel.grid.minor = element_blank(), \r\n        legend.position = \"none\")\r\n\r\n\r\n\r\n\r\n\r\nsummary(data_month_bi)\r\n\r\n       item_no           week           sold        \r\n 020-307   :   54   Min.   : 0.0   Min.   :   0.00  \r\n 020-502   :   54   1st Qu.:13.0   1st Qu.:   0.00  \r\n 025-207   :   54   Median :26.5   Median :   0.00  \r\n 02FR182024:   54   Mean   :26.5   Mean   :  50.62  \r\n 04002032  :   54   3rd Qu.:40.0   3rd Qu.:   2.00  \r\n 04120002  :   54   Max.   :53.0   Max.   :7200.00  \r\n (Other)   :26028                                   \r\n       Month        sold_binary    \r\n July     : 2440   Min.   :0.0000  \r\n March    : 2440   1st Qu.:0.0000  \r\n June     : 2440   Median :0.0000  \r\n August   : 1952   Mean   :0.2516  \r\n September: 1952   3rd Qu.:1.0000  \r\n October  : 1952   Max.   :1.0000  \r\n (Other)  :13176                   \r\n\r\n\r\n\r\n#install.packages(\"caTools\")\r\nlibrary(caTools)\r\nset.seed(123)\r\nsplit_data <- sample.split(data_month_bi$sold, SplitRatio = 0.7)\r\n\r\ntest_data <- subset(data_month_bi, split_data == FALSE)\r\n\r\ntrain_data <- subset(data_month_bi, split_data == TRUE)\r\n\r\n\r\n\r\n\r\n#model 1\r\nfit <- glm(sold_binary ~ Month + item_no, data=train_data, family=binomial())\r\n\r\n\r\n\r\n\r\nset.seed(123)\r\nsplit_data2 <- sample.split(data_month$sold, SplitRatio = 0.7)\r\n\r\ntest_data2 <- subset(data_month, split_data2 == FALSE)\r\n\r\ntrain_data2 <- subset(data_month, split_data2 == TRUE)\r\n\r\nfit2 <- glm(sold ~ Month + item_no, data = train_data2)\r\n\r\n\r\n\r\n\r\nlibrary(broom)\r\n\r\nmodel <- tidy(fit)\r\noptions(scipen = 999)\r\nmodel\r\n\r\n# A tibble: 500 × 5\r\n   term           estimate std.error statistic  p.value\r\n   <chr>             <dbl>     <dbl>     <dbl>    <dbl>\r\n 1 (Intercept)     -1.43      0.425     -3.35  0.000796\r\n 2 MonthAugust      0.135     0.0986     1.37  0.170   \r\n 3 MonthSeptember  -0.121     0.101     -1.20  0.232   \r\n 4 MonthOctober    -0.0883    0.101     -0.875 0.382   \r\n 5 MonthNovember    0.132     0.0986     1.34  0.180   \r\n 6 MonthDecember   -0.317     0.104     -3.03  0.00244 \r\n 7 MonthJanuary     0.122     0.0989     1.23  0.217   \r\n 8 MonthFebruary    0.0330    0.0986     0.335 0.738   \r\n 9 MonthMarch       0.151     0.0929     1.63  0.103   \r\n10 MonthApril      -0.124     0.101     -1.22  0.222   \r\n# ℹ 490 more rows\r\n\r\n\r\n\r\nsig <- subset(model, p.value < 0.05)\r\nprint(sig, n = 10)\r\n\r\n# A tibble: 114 × 5\r\n   term                estimate std.error statistic    p.value\r\n   <chr>                  <dbl>     <dbl>     <dbl>      <dbl>\r\n 1 (Intercept)           -1.43     0.425      -3.35 0.000796  \r\n 2 MonthDecember         -0.317    0.104      -3.03 0.00244   \r\n 3 MonthJune             -0.284    0.0971     -2.93 0.00339   \r\n 4 item_no02FR182024     -2.20     1.10       -2.01 0.0449    \r\n 5 item_no10055.011010    2.07     0.536       3.86 0.000113  \r\n 6 item_no10055.011620    2.48     0.560       4.43 0.00000957\r\n 7 item_no10055.011625    1.54     0.518       2.98 0.00288   \r\n 8 item_no10055.012020    1.69     0.545       3.10 0.00193   \r\n 9 item_no10055.012025    1.71     0.526       3.24 0.00118   \r\n10 item_no10055.021625    1.43     0.535       2.67 0.00765   \r\n# ℹ 104 more rows\r\n\r\n\r\n\r\n#predictions\r\ntest_data$prob <- predict(fit, test_data, type=\"response\")\r\n\r\ntest_data$pred <- test_data %>% mutate(ifelse(prob > .75, \"1\", \"0\"))\r\n\r\n\r\n\r\n\r\naccuracy <- mean(head(test_data$pred$`ifelse(prob > 0.75, \"1\", \"0\")`, 35)\r\n                 == head(test_data$sold_binary, 35))\r\naccuracy #77% accuracy\r\n\r\n[1] 0.7714286\r\n\r\n\r\n\r\n#predictions for fit2\r\ntest_data2$prob <- predict(fit2, test_data2, type=\"response\")\r\n\r\ntest_data2$pred <- test_data2 %>% mutate(ifelse(prob > .75, \"1\", \"0\"))\r\n\r\n\r\n\r\n\r\naccuracy2 <- mean(head(test_data2$pred$`ifelse(prob > 0.75, \"1\", \"0\")`, 35)\r\n                  == head(test_data2$sold, 35))\r\n\r\n\r\n\r\n\r\n#what is our second model predicting at?\r\naccuracy2 #2% accuracy\r\n\r\n[1] 0.02857143\r\n\r\nOur first model is predicting with 77% accuracy\r\nDiscussion & Reflection\r\nMini competition 2 set the goal to create a model to complete the task of projecting a company’s sales of widgets and then communicate our findings to a general audience. In this example, we utilized a generalized linear model approach to accomplish this task and framed the output in terms of months of projected high sales. This way, the company was able to use our predictions to choose which widgets should be kept in excess stock during specific months. One of the main goals of this assignment was to intentionally frame the output and predictions in a manner that is easily digestible to people who are not formally trained in statistics. A challenge of this goal was to strike a balance between effective communication without condescension- this is often a critique which non-academics have with academia.\r\nOne problem with our approach to the generalized linear model was with our training and testing models. While we randomized the dataset and split our data into a 70:30 training and testing sets, this ignores the collinearity of the data. For example, it is possible that one widget over multiple months could have been selected for either the training or testing sets. To ensure an even separation of widgets, a better approach would have been to select by unique widget and then subset the data. This project stretched my thinking about a problem, not only in what approach to use, but also how to effectively communicate the findings to a general audience\r\nCourse Objective 3\r\nConduct model selection for a set of candidate models\r\nFrom Mini Competition 1\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(broom)\r\nlibrary(ggplot2)\r\nlibrary(tidymodels)\r\nlibrary(GGally)\r\nlibrary(olsrr)\r\n\r\n\r\n\r\n\r\nchallenge_data <- readr::read_csv(\"2019_data.csv\")\r\n\r\n\r\n\r\n\r\nselected_vars <- c(\"FSSPORTX\", \"FSVOL\", \"FSMTNG\", \"FSPTMTNG\", \"FSFUNDRS\", \r\n                   \"FSCOMMTE\", \"FSCOUNSLR\", \"FSFREQ\",\"FSATCNFN\", \"FHCHECKX\", \r\n                   \"FHHELP\", \"FHPLACE\", \"SCCHOICE\", \"SPUBCHOIX\",  \"SEGRADES\", \r\n                   \"SCONSIDR\")\r\n\r\n\r\n\r\n\r\n#create a new data set\r\nnew_data <- challenge_data[, selected_vars]\r\n\r\n\r\n\r\n\r\nfiltered_data <- new_data %>%\r\n  filter(if_all(c(\"FSSPORTX\", \"FSVOL\", \"FSMTNG\", \"FSPTMTNG\", \"FSFUNDRS\",\r\n                  \"FSCOMMTE\",\"FSCOUNSLR\",\"FSATCNFN\", \"FHPLACE\", \"SCCHOICE\",\r\n                  \"SCONSIDR\", \"SPUBCHOIX\"), ~. %in% c(1, 2)))\r\n\r\n\r\n\r\n\r\nfiltered_data2 <- filtered_data %>%\r\n  mutate(across(c(\"FSSPORTX\", \"FSVOL\", \"FSMTNG\", \"FSPTMTNG\", \"FSFUNDRS\", \r\n                  \"FSCOMMTE\", \"FSCOUNSLR\", \"FSATCNFN\", \"FHPLACE\", \"SCCHOICE\", \r\n                  \"SPUBCHOIX\", \"SCONSIDR\"), ~ifelse(. == 1, \"Yes\", \"No\")))\r\n\r\n\r\n\r\n\r\nfiltered_data3 <- filtered_data2 %>%\r\n  mutate(FHHELP = case_when(\r\n    FHHELP %in% c(1, 2) ~ \"few\",  # Combine 1's and 2's into 1 (pass)\r\n    FHHELP %in% c(3, 4) ~ \"many\",   # Combine 3's and 4's into 0 (fail)\r\n    TRUE ~ NA_character_            # Set other values (including 5) to NA\r\n  )) %>%\r\n  filter(!is.na(FHHELP)) \r\n\r\n\r\n\r\n\r\nfiltered_data4 <- filtered_data3 %>%\r\n  mutate(FHCHECKX = case_when(\r\n    FHCHECKX %in% c(1, 2) ~ \"few\",  # Combine 1's and 2's into 1 (pass)\r\n    FHCHECKX %in% c(3, 4) ~ \"many\",   # Combine 3's and 4's into 0 (fail)\r\n    TRUE ~ NA_character_            # Set other values (including 5) to NA\r\n  )) %>%\r\n  filter(!is.na(FHCHECKX)) \r\n\r\n\r\n\r\n\r\n## Splitting data into train and test and fitting the model\r\n# set seed before random split\r\nset.seed(40)\r\n# put 80% of the data into the training set\r\ndata_split <- initial_split(filtered_data4, prop = 0.80)\r\n\r\n# assign the two splits to data frames - with descriptive names\r\ndata_train <- training(data_split)\r\ndata_test <- testing(data_split)\r\n\r\n# splits\r\ndata_train\r\n\r\n# A tibble: 7,407 × 16\r\n   FSSPORTX FSVOL FSMTNG FSPTMTNG FSFUNDRS FSCOMMTE FSCOUNSLR FSFREQ\r\n   <chr>    <chr> <chr>  <chr>    <chr>    <chr>    <chr>      <dbl>\r\n 1 Yes      No    Yes    Yes      Yes      No       No            20\r\n 2 Yes      Yes   Yes    Yes      Yes      Yes      Yes            8\r\n 3 Yes      No    Yes    No       Yes      No       Yes            5\r\n 4 Yes      Yes   Yes    Yes      Yes      No       Yes           10\r\n 5 Yes      No    Yes    Yes      Yes      No       Yes            3\r\n 6 Yes      Yes   Yes    Yes      Yes      Yes      Yes           10\r\n 7 Yes      No    Yes    Yes      Yes      No       No             1\r\n 8 No       No    No     No       No       No       No             2\r\n 9 No       No    No     Yes      No       No       No             1\r\n10 Yes      Yes   Yes    Yes      Yes      Yes      No            14\r\n# ℹ 7,397 more rows\r\n# ℹ 8 more variables: FSATCNFN <chr>, FHCHECKX <chr>, FHHELP <chr>,\r\n#   FHPLACE <chr>, SCCHOICE <chr>, SPUBCHOIX <chr>, SEGRADES <dbl>,\r\n#   SCONSIDR <chr>\r\n\r\nFitting the model\r\n\r\n\r\n#fit the mlr model\r\nlm_spec <- linear_reg() %>%\r\nset_mode(\"regression\") %>%\r\nset_engine(\"lm\")\r\n\r\ninitial_model <- lm_spec %>% \r\nfit(SEGRADES ~ FSSPORTX+FSVOL+FSMTNG+FSPTMTNG+FSATCNFN+FSFUNDRS+FSCOMMTE+\r\n      FSCOUNSLR+FSFREQ+SCCHOICE+SPUBCHOIX+FHCHECKX+FHHELP+FHPLACE+\r\n      FSMTNG*FHHELP+FSPTMTNG*FHHELP+SCONSIDR, data = data_train)\r\n\r\nstats<- tidy(initial_model)\r\n\r\n\r\nFinal models\r\n\r\n\r\nfinal_model1 <- lm_spec %>% \r\nfit(SEGRADES ~ FSSPORTX+FSATCNFN+FHCHECKX+FHHELP+FSMTNG*FHHELP+FSMTNG+\r\n      FSCOMMTE+SCCHOICE+FHPLACE, data = data_train)\r\n\r\ntidy(final_model1)\r\n\r\n# A tibble: 10 × 5\r\n   term                 estimate std.error statistic   p.value\r\n   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\r\n 1 (Intercept)            1.81      0.0776    23.3   1.15e-115\r\n 2 FSSPORTXYes           -0.170     0.0415    -4.09  4.44e-  5\r\n 3 FSATCNFNYes            0.410     0.0365    11.2   4.40e- 29\r\n 4 FHCHECKXmany           0.425     0.0480     8.84  1.16e- 18\r\n 5 FHHELPmany             0.0912    0.102      0.895 3.71e-  1\r\n 6 FSMTNGYes             -0.261     0.0569    -4.58  4.66e-  6\r\n 7 FSCOMMTEYes           -0.178     0.0429    -4.14  3.55e-  5\r\n 8 SCCHOICEYes           -0.0986    0.0308    -3.20  1.38e-  3\r\n 9 FHPLACEYes            -0.207     0.0441    -4.69  2.76e-  6\r\n10 FHHELPmany:FSMTNGYes   0.232     0.107      2.18  2.96e-  2\r\n\r\n\r\n\r\n#final model chosen after the backward selection using AIC values\r\nfinal_model2 <- lm_spec %>% \r\nfit(SEGRADES ~ FSSPORTX+FSATCNFN+FHCHECKX+FHHELP+FSMTNG*FHHELP+FSMTNG+\r\n      FSCOMMTE+SCCHOICE+FHPLACE+FSFUNDRS, data = data_train)\r\n\r\ntidy(final_model2)\r\n\r\n# A tibble: 11 × 5\r\n   term                 estimate std.error statistic   p.value\r\n   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\r\n 1 (Intercept)            1.81      0.0776    23.3   6.23e-116\r\n 2 FSSPORTXYes           -0.153     0.0425    -3.60  3.19e-  4\r\n 3 FSATCNFNYes            0.415     0.0366    11.4   1.25e- 29\r\n 4 FHCHECKXmany           0.424     0.0480     8.84  1.19e- 18\r\n 5 FHHELPmany             0.0941    0.102      0.924 3.56e-  1\r\n 6 FSMTNGYes             -0.248     0.0574    -4.32  1.60e-  5\r\n 7 FSCOMMTEYes           -0.162     0.0438    -3.69  2.24e-  4\r\n 8 SCCHOICEYes           -0.0963    0.0308    -3.13  1.78e-  3\r\n 9 FHPLACEYes            -0.204     0.0441    -4.64  3.62e-  6\r\n10 FSFUNDRSYes           -0.0608    0.0331    -1.84  6.59e-  2\r\n11 FHHELPmany:FSMTNGYes   0.231     0.107      2.17  3.00e-  2\r\n\r\nfinal_model1\r\n\r\n\r\n## Making  predictions and check for accuracy of predictions\r\npredictions <- predict(final_model1, new_data = data_test)\r\n\r\n\r\n\r\n\r\n#check accuracy of the  predictions\r\ntrue_labels <- data_test$SEGRADES\r\npredictions_round <- round(predictions)# rounding to get whole numbers instea of decimals\r\naccuracy1 <- sum(predictions_round == true_labels) / length(true_labels)\r\naccuracy1\r\n\r\n[1] 0.3439525\r\n\r\nfinal_model2\r\n\r\n\r\npredictions2 <- predict(final_model2, new_data = data_test)\r\n\r\n\r\n\r\n\r\ntrue_labels <- data_test$SEGRADES\r\npredictions2_round <- round(predictions2)\r\naccuracy2 <- sum(predictions2_round == true_labels) / length(true_labels)\r\naccuracy2\r\n\r\n[1] 0.3363931\r\n\r\nDiscussion & Reflection\r\nThe goal of the first mini competition was to try and assess which factors contribute to a students’ academic success. We were given a very large data set and a big challenge was to pare down which factors to even use in the model. We used a backward selection model to help with this process and had several iterations of our model run. We then compared the accuracy of a few of the final models and chose the model with the highest accuracy (which was still only 34%).\r\nOne challenge with this assignment was the code itself. Since I was the one presenting, I needed to go over and make sure that the code was correctly written and the models ran as expected. One thing I learned to appreciate was how each person tackles a given problem differently than how I would initially approach it, and we are all different when it comes to code organization. Reading another person’s code and needing to make changes to it when necessary really helped my own debugging skills and shed light on the necessity for comments. Overall, it was a fun but challenging assignment.\r\nCourse Objective 4\r\nCommunicate the results of statistical models to a general audience\r\nFrom Mini Competition 2 - see code above\r\n\r\n\r\nknitr::include_graphics(\"images/comp2_graph.jpg\")\r\n\r\n\r\n\r\nFigure 1: Graph illustrating predictions in sales\r\n\r\n\r\n\r\n\r\n\r\nknitr::include_graphics(\"images/comp2.jpg\")\r\n\r\n\r\n\r\nFigure 2: Communicating findings to our audience\r\n\r\n\r\n\r\nDiscussion & Reflection\r\nAs mentioned above, a main goal of this project was to use a model to predict sales of products, then communicate those predictions to a general audience. While I was not the one to give the presentation, I did play a major role in the development of the models as well as creating the presentation. It was difficult to find a balance between effectively and accurately communicating what was done, why it was done, and how it was done in a manner that makes sense to non-statisticians. This is something that I know I struggle with, as I tend to drone and give too many unimportant details when storytelling. Working on this assignment in a group helped me to see the level of effective communication that can occur, even with complicated topics.\r\nAdditionally, seeing other groups present their information and how they approached the assignment was equally insightful. One group in particular, I recall had the most visually impactful presentation and looked incredibly professional. I felt that they did a great job communicating their work in an effective way, without diminishing their own efforts or demeaning their audience.\r\nCourse Objective 5\r\nUse programming software (i.e., R) to fit and assess statistical models\r\nSee above for examples of R code\r\nDiscussion & Reflection\r\nBetween the multiple assignments and mini competitions in this class, we used R for all analyses. I am constantly amazed at the growth in this language that I continue to develop during my studies. The books, videos, and lectures all taught me new skills in R and these are tools which I will take with me as my career progresses. The amount of effort in this course to succeed was self-driven. In my opinion, these topics are of the sort that one will continue to learn new ways to approach them, so the code that I write today will be very different from the code I write in the future. Additionally, working on the mini competitions with other people helped me see how other people attack a problem in R, starting with how the clean the data, organize their code, and make final interpretations. Reading other people’s code helped me to realize how I struggle with comments in my own code and how I need to improve on that aspect.\r\nOverall, this class was a lot to keep up with between readings, videos, and assignments, but I believe that I have the tools to continue my learning journey from here. I particularly enjoyed many of the in-class discussions with my peers - we all come from such different backgrounds in studies and life that it was insightful to hear different perspectives on various topics such as AI, data feminism, or career aspirations. Sometimes it’s easy to feel alone on your own journey, so I think finding common ground among your peers is important and brings a sense of community into your life.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-08T01:20:38-04:00"
    },
    {
      "path": "index.html",
      "title": "Rachel Orzechowski",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          The Caffeinated Cavy\r\n          \r\n          \r\n          Home\r\n          \r\n          \r\n          Projects\r\n           \r\n          ▾\r\n          \r\n          \r\n          Current\r\n          Past\r\n          \r\n          \r\n          \r\n          \r\n          Resources\r\n           \r\n          ▾\r\n          \r\n          \r\n          Blogs\r\n          Books\r\n          Binge Watch\r\n          \r\n          \r\n          Gallery\r\n          \r\n          \r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            Rachel Orzechowski\r\n          \r\n          \r\n            \r\n              I am an aquatic ecologist specializing in dreissenid\r\n              mussels based in the Great Lakes region. Currently, I work\r\n              with the Elgin Lab as a NOAA Affiliate research assistant.\r\n              I enjoy hiking, camping, and drinking too much coffee.\r\n              Navigate through the site to see\r\n              current\r\n              and\r\n              past\r\n              projects and topics that I have worked on.\r\n              Utilize the Resources button for helpful\r\n              blogs,\r\n              books,\r\n              or\r\n              binge\r\n              watch videos to expand your knowledge on statistics,\r\n              data science, the R programming language, and more.\r\n              Peruse the\r\n              photo\r\n              gallery to see what a career as a research scientist\r\n              might look like.\r\n              \r\n              Want More?\r\n              \r\n              Click a button below\r\n              \r\n              \r\n            \r\n            \r\n              I am an aquatic ecologist specializing in dreissenid\r\n              mussels based in the Great Lakes region. Currently, I work\r\n              with the Elgin Lab as a NOAA Affiliate research assistant.\r\n              I enjoy hiking, camping, and drinking too much coffee.\r\n              Navigate through the site to see\r\n              current\r\n              and\r\n              past\r\n              projects and topics that I have worked on.\r\n              Utilize the Resources button for helpful\r\n              blogs,\r\n              books,\r\n              or\r\n              binge\r\n              watch videos to expand your knowledge on statistics,\r\n              data science, the R programming language, and more.\r\n              Peruse the\r\n              photo\r\n              gallery to see what a career as a research scientist\r\n              might look like.\r\n              \r\n              Want More?\r\n              \r\n              Click a button below\r\n              \r\n              \r\n            \r\n          \r\n\r\n          \r\n            \r\n              \r\n                  \r\n                    \r\n                      LinkedIn\r\n                    \r\n                  \r\n                \r\n                                \r\n                  \r\n                    \r\n                      NOAA\r\n                    \r\n                  \r\n                \r\n                                \r\n                  \r\n                    \r\n                      GitHub\r\n                    \r\n                  \r\n                \r\n                                \r\n                  \r\n                    \r\n                      Email\r\n                    \r\n                  \r\n                \r\n                              \r\n          \r\n\r\n          \r\n            \r\n              \r\n                                \r\n                  \r\n                    LinkedIn\r\n                  \r\n                \r\n                                \r\n                  \r\n                    NOAA\r\n                  \r\n                \r\n                                \r\n                  \r\n                    GitHub\r\n                  \r\n                \r\n                                \r\n                  \r\n                    Email\r\n                  \r\n                \r\n                              \r\n            \r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2023-08-06T11:59:50-04:00"
    },
    {
      "path": "past_projects.html",
      "title": "Past Projects",
      "description": "A recap of topics covered in previous coursework\n",
      "author": [],
      "contents": "\r\nCIS 635 - Summer 2023\r\nCIS 671- Winter 2023\r\nSTA 518- Summer 2022\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-06T02:51:35-04:00"
    },
    {
      "path": "photo_gallery.html",
      "title": "Photo Gallery",
      "description": "Photos from past projects\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\nFigure 1: Dreissena bugensis siphons\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 2: Stenacron spp. under a microscope\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Canadian Coast Guard vessel Limnos\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 4: Night shift on the CCG Limnos\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-07T22:44:29-04:00"
    },
    {
      "path": "video_list.html",
      "title": "Videos",
      "description": "Helpful videos on topics including statistics, R tuturials, and more!\n",
      "author": [],
      "contents": "\r\nStatQuest with Josh StarmerClearly explained statistical topics\r\nJenny SloaneEasy to follow distill tutorials\r\nData Science AnalyticsAuthors of ISLR covering various topics in their book\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-06T23:33:02-04:00"
    }
  ],
  "collections": []
}
